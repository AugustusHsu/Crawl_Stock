{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the 0.h5 model\n",
      "> Compilation Time :  0.027168989181518555\n",
      "Train on 2779 samples, validate on 309 samples\n",
      "Epoch 1/5\n",
      "2779/2779 [==============================] - 4s - loss: 11.5067 - val_loss: 12.9941\n",
      "Epoch 2/5\n",
      "2779/2779 [==============================] - 2s - loss: 10.4226 - val_loss: 11.5802\n",
      "Epoch 3/5\n",
      "2779/2779 [==============================] - 2s - loss: 9.7341 - val_loss: 10.8526\n",
      "Epoch 4/5\n",
      "2779/2779 [==============================] - 2s - loss: 9.2450 - val_loss: 10.1600\n",
      "Epoch 5/5\n",
      "2779/2779 [==============================] - 2s - loss: 8.8345 - val_loss: 9.4804\n",
      "Training duration (s) :  21.31632399559021\n",
      "Train the 1.h5 model\n",
      "> Compilation Time :  0.029911041259765625\n",
      "Train on 2772 samples, validate on 309 samples\n",
      "Epoch 1/5\n",
      "2772/2772 [==============================] - 4s - loss: 11.3407 - val_loss: 9.7434\n",
      "Epoch 2/5\n",
      "2772/2772 [==============================] - 2s - loss: 10.3840 - val_loss: 8.7950\n",
      "Epoch 3/5\n",
      "2772/2772 [==============================] - 2s - loss: 9.8675 - val_loss: 8.3204\n",
      "Epoch 4/5\n",
      "2772/2772 [==============================] - 2s - loss: 9.3825 - val_loss: 7.8582\n",
      "Epoch 5/5\n",
      "2772/2772 [==============================] - 2s - loss: 9.0345 - val_loss: 7.4802\n",
      "Training duration (s) :  21.507937908172607\n",
      "Train the 2.h5 model\n",
      "> Compilation Time :  0.027904033660888672\n",
      "Train on 2758 samples, validate on 307 samples\n",
      "Epoch 1/5\n",
      "2758/2758 [==============================] - 4s - loss: 12.4502 - val_loss: 9.5962\n",
      "Epoch 2/5\n",
      "2758/2758 [==============================] - 2s - loss: 11.7099 - val_loss: 8.7185\n",
      "Epoch 3/5\n",
      "2758/2758 [==============================] - 2s - loss: 11.0108 - val_loss: 8.2792\n",
      "Epoch 4/5\n",
      "2758/2758 [==============================] - 2s - loss: 10.5274 - val_loss: 7.7964\n",
      "Epoch 5/5\n",
      "2758/2758 [==============================] - 2s - loss: 10.1248 - val_loss: 7.4441\n",
      "Training duration (s) :  21.95133399963379\n",
      "Train the 3.h5 model\n",
      "> Compilation Time :  0.02771615982055664\n",
      "Train on 2752 samples, validate on 306 samples\n",
      "Epoch 1/5\n",
      "2752/2752 [==============================] - 7s - loss: 14.3413 - val_loss: 22.2576\n",
      "Epoch 2/5\n",
      "2752/2752 [==============================] - 5s - loss: 12.6209 - val_loss: 20.4911\n",
      "Epoch 3/5\n",
      "2752/2752 [==============================] - 5s - loss: 11.6845 - val_loss: 19.4549\n",
      "Epoch 4/5\n",
      "2752/2752 [==============================] - 5s - loss: 10.8106 - val_loss: 18.5289\n",
      "Epoch 5/5\n",
      "2752/2752 [==============================] - 5s - loss: 10.0792 - val_loss: 17.6664\n",
      "Training duration (s) :  37.126083850860596\n",
      "Train the 4.h5 model\n",
      "> Compilation Time :  0.028011083602905273\n",
      "Train on 2745 samples, validate on 306 samples\n",
      "Epoch 1/5\n",
      "2745/2745 [==============================] - 7s - loss: 15.1335 - val_loss: 16.2694\n",
      "Epoch 2/5\n",
      "2745/2745 [==============================] - 5s - loss: 14.0498 - val_loss: 14.8234\n",
      "Epoch 3/5\n",
      "2745/2745 [==============================] - 5s - loss: 12.9598 - val_loss: 13.9495\n",
      "Epoch 4/5\n",
      "2745/2745 [==============================] - 5s - loss: 12.2150 - val_loss: 12.9321\n",
      "Epoch 5/5\n",
      "2745/2745 [==============================] - 5s - loss: 11.3326 - val_loss: 12.1755\n",
      "Training duration (s) :  37.543920040130615\n",
      "Train the 5.h5 model\n",
      "> Compilation Time :  0.3470149040222168\n",
      "Train on 2731 samples, validate on 304 samples\n",
      "Epoch 1/5\n",
      "2731/2731 [==============================] - 7s - loss: 14.6837 - val_loss: 16.5228\n",
      "Epoch 2/5\n",
      "2731/2731 [==============================] - 5s - loss: 13.8733 - val_loss: 15.3955\n",
      "Epoch 3/5\n",
      "2731/2731 [==============================] - 5s - loss: 12.9482 - val_loss: 14.7781\n",
      "Epoch 4/5\n",
      "2731/2731 [==============================] - 5s - loss: 12.1952 - val_loss: 13.8938\n",
      "Epoch 5/5\n",
      "2731/2731 [==============================] - 5s - loss: 11.4781 - val_loss: 13.1940\n",
      "Training duration (s) :  38.080926179885864\n",
      "Train the 6.h5 model\n",
      "> Compilation Time :  0.03246188163757324\n",
      "Train on 2725 samples, validate on 303 samples\n",
      "Epoch 1/5\n",
      "2725/2725 [==============================] - 11s - loss: 10.8061 - val_loss: 5.7887\n",
      "Epoch 2/5\n",
      "2725/2725 [==============================] - 8s - loss: 9.1497 - val_loss: 4.8026\n",
      "Epoch 3/5\n",
      "2725/2725 [==============================] - 8s - loss: 8.2699 - val_loss: 4.2457\n",
      "Epoch 4/5\n",
      "2725/2725 [==============================] - 9s - loss: 7.5128 - val_loss: 3.9697\n",
      "Epoch 5/5\n",
      "2725/2725 [==============================] - 9s - loss: 7.0041 - val_loss: 3.8787\n",
      "Training duration (s) :  54.657243967056274\n",
      "Train the 7.h5 model\n",
      "> Compilation Time :  0.027584075927734375\n",
      "Train on 2718 samples, validate on 303 samples\n",
      "Epoch 1/5\n",
      "2718/2718 [==============================] - 11s - loss: 11.4295 - val_loss: 10.4470\n",
      "Epoch 2/5\n",
      "2718/2718 [==============================] - 8s - loss: 10.3288 - val_loss: 9.3943\n",
      "Epoch 3/5\n",
      "2718/2718 [==============================] - 8s - loss: 9.2808 - val_loss: 8.8170\n",
      "Epoch 4/5\n",
      "2718/2718 [==============================] - 8s - loss: 8.5613 - val_loss: 8.2103\n",
      "Epoch 5/5\n",
      "2718/2718 [==============================] - 8s - loss: 8.0309 - val_loss: 7.7898\n",
      "Training duration (s) :  54.746620178222656\n",
      "Train the 8.h5 model\n",
      "> Compilation Time :  0.028989076614379883\n",
      "Train on 2704 samples, validate on 301 samples\n",
      "Epoch 1/5\n",
      "2704/2704 [==============================] - 11s - loss: 13.1254 - val_loss: 12.7192\n",
      "Epoch 2/5\n",
      "2704/2704 [==============================] - 8s - loss: 12.1302 - val_loss: 11.5043\n",
      "Epoch 3/5\n",
      "2704/2704 [==============================] - 8s - loss: 11.1378 - val_loss: 10.8297\n",
      "Epoch 4/5\n",
      "2704/2704 [==============================] - 8s - loss: 10.4134 - val_loss: 10.1560\n",
      "Epoch 5/5\n",
      "2704/2704 [==============================] - 8s - loss: 9.9136 - val_loss: 9.7224\n",
      "Training duration (s) :  53.90701413154602\n",
      "Train the 9.h5 model\n",
      "> Compilation Time :  0.03086996078491211\n",
      "Train on 2644 samples, validate on 294 samples\n",
      "Epoch 1/5\n",
      "2644/2644 [==============================] - 19s - loss: 29.6368 - val_loss: 21.2394\n",
      "Epoch 2/5\n",
      "2644/2644 [==============================] - 17s - loss: 26.6072 - val_loss: 18.3203\n",
      "Epoch 3/5\n",
      "2644/2644 [==============================] - 16s - loss: 24.0276 - val_loss: 17.1096\n",
      "Epoch 4/5\n",
      "2644/2644 [==============================] - 16s - loss: 22.2800 - val_loss: 15.3609\n",
      "Epoch 5/5\n",
      "2644/2644 [==============================] - 17s - loss: 20.3106 - val_loss: 13.9816\n",
      "Training duration (s) :  97.2498230934143\n",
      "Train the 10.h5 model\n",
      "> Compilation Time :  0.038343191146850586\n",
      "Train on 2637 samples, validate on 294 samples\n",
      "Epoch 1/5\n",
      "2637/2637 [==============================] - 20s - loss: 30.3915 - val_loss: 25.7251\n",
      "Epoch 2/5\n",
      "2637/2637 [==============================] - 16s - loss: 28.1022 - val_loss: 23.1123\n",
      "Epoch 3/5\n",
      "2637/2637 [==============================] - 16s - loss: 25.6512 - val_loss: 21.7977\n",
      "Epoch 4/5\n",
      "2637/2637 [==============================] - 16s - loss: 24.0254 - val_loss: 19.7935\n",
      "Epoch 5/5\n",
      "2637/2637 [==============================] - 16s - loss: 22.2030 - val_loss: 18.3430\n",
      "Training duration (s) :  98.6795301437378\n",
      "Train the 11.h5 model\n",
      "> Compilation Time :  0.02724289894104004\n",
      "Train on 2623 samples, validate on 292 samples\n",
      "Epoch 1/5\n",
      "2623/2623 [==============================] - 19s - loss: 30.3406 - val_loss: 30.5547\n",
      "Epoch 2/5\n",
      "2623/2623 [==============================] - 17s - loss: 28.5077 - val_loss: 27.9693\n",
      "Epoch 3/5\n",
      "2623/2623 [==============================] - 17s - loss: 26.5194 - val_loss: 26.2529\n",
      "Epoch 4/5\n",
      "2623/2623 [==============================] - 17s - loss: 24.9663 - val_loss: 24.4366\n",
      "Epoch 5/5\n",
      "2623/2623 [==============================] - 17s - loss: 23.1377 - val_loss: 22.7980\n",
      "Training duration (s) :  98.58435893058777\n",
      "Train the 12.h5 model\n",
      "> Compilation Time :  0.02730107307434082\n",
      "Train on 5922 samples, validate on 658 samples\n",
      "Epoch 1/5\n",
      "5922/5922 [==============================] - 9s - loss: 2.2416 - val_loss: 1.3642\n",
      "Epoch 2/5\n",
      "5922/5922 [==============================] - 6s - loss: 1.7251 - val_loss: 1.2276\n",
      "Epoch 3/5\n",
      "5922/5922 [==============================] - 6s - loss: 1.5437 - val_loss: 1.1359\n",
      "Epoch 4/5\n",
      "5922/5922 [==============================] - 6s - loss: 1.4335 - val_loss: 1.0909\n",
      "Epoch 5/5\n",
      "5922/5922 [==============================] - 6s - loss: 1.3706 - val_loss: 1.0647\n",
      "Training duration (s) :  41.77595806121826\n",
      "Train the 13.h5 model\n",
      "> Compilation Time :  0.027420759201049805\n",
      "Train on 5915 samples, validate on 658 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5915/5915 [==============================] - 8s - loss: 2.2470 - val_loss: 1.4964\n",
      "Epoch 2/5\n",
      "5915/5915 [==============================] - 5s - loss: 1.8520 - val_loss: 1.3690\n",
      "Epoch 3/5\n",
      "5915/5915 [==============================] - 5s - loss: 1.7017 - val_loss: 1.2788\n",
      "Epoch 4/5\n",
      "5915/5915 [==============================] - 5s - loss: 1.6310 - val_loss: 1.2486\n",
      "Epoch 5/5\n",
      "5915/5915 [==============================] - 5s - loss: 1.5747 - val_loss: 1.2193\n",
      "Training duration (s) :  38.64078092575073\n",
      "Train the 14.h5 model\n",
      "> Compilation Time :  0.027272939682006836\n",
      "Train on 5901 samples, validate on 656 samples\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 9s - loss: 2.4337 - val_loss: 2.2281\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 6s - loss: 2.0756 - val_loss: 1.9680\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 5s - loss: 1.9050 - val_loss: 1.8138\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 6s - loss: 1.8330 - val_loss: 1.7672\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 6s - loss: 1.7822 - val_loss: 1.7609\n",
      "Training duration (s) :  43.47643995285034\n",
      "Train the 15.h5 model\n",
      "> Compilation Time :  0.031527042388916016\n",
      "Train on 5895 samples, validate on 655 samples\n",
      "Epoch 1/5\n",
      "5895/5895 [==============================] - 15s - loss: 2.3429 - val_loss: 1.8903\n",
      "Epoch 2/5\n",
      "5895/5895 [==============================] - 12s - loss: 1.7547 - val_loss: 1.6014\n",
      "Epoch 3/5\n",
      "5895/5895 [==============================] - 12s - loss: 1.5482 - val_loss: 1.3881\n",
      "Epoch 4/5\n",
      "5895/5895 [==============================] - 12s - loss: 1.4502 - val_loss: 1.3119\n",
      "Epoch 5/5\n",
      "5895/5895 [==============================] - 12s - loss: 1.3943 - val_loss: 1.2600\n",
      "Training duration (s) :  74.89679002761841\n",
      "Train the 16.h5 model\n",
      "> Compilation Time :  0.026958227157592773\n",
      "Train on 5888 samples, validate on 655 samples\n",
      "Epoch 1/5\n",
      "5888/5888 [==============================] - 15s - loss: 2.5175 - val_loss: 2.2572\n",
      "Epoch 2/5\n",
      "5888/5888 [==============================] - 12s - loss: 1.9526 - val_loss: 1.8982\n",
      "Epoch 3/5\n",
      "5888/5888 [==============================] - 12s - loss: 1.7270 - val_loss: 1.7472\n",
      "Epoch 4/5\n",
      "5888/5888 [==============================] - 12s - loss: 1.6092 - val_loss: 1.7245\n",
      "Epoch 5/5\n",
      "5888/5888 [==============================] - 12s - loss: 1.5506 - val_loss: 1.6784\n",
      "Training duration (s) :  75.44875407218933\n",
      "Train the 17.h5 model\n",
      "> Compilation Time :  0.03467297554016113\n",
      "Train on 5874 samples, validate on 653 samples\n",
      "Epoch 1/5\n",
      "5874/5874 [==============================] - 16s - loss: 3.1407 - val_loss: 2.4852\n",
      "Epoch 2/5\n",
      "5874/5874 [==============================] - 12s - loss: 2.5993 - val_loss: 2.1879\n",
      "Epoch 3/5\n",
      "5874/5874 [==============================] - 12s - loss: 2.3153 - val_loss: 2.0056\n",
      "Epoch 4/5\n",
      "5874/5874 [==============================] - 12s - loss: 2.1821 - val_loss: 1.9336\n",
      "Epoch 5/5\n",
      "5874/5874 [==============================] - 12s - loss: 2.1463 - val_loss: 1.9012\n",
      "Training duration (s) :  80.77297306060791\n",
      "Train the 18.h5 model\n",
      "> Compilation Time :  0.031093835830688477\n",
      "Train on 5868 samples, validate on 652 samples\n",
      "Epoch 1/5\n",
      "5868/5868 [==============================] - 23s - loss: 3.8592 - val_loss: 2.8902\n",
      "Epoch 2/5\n",
      "5868/5868 [==============================] - 19s - loss: 3.0169 - val_loss: 2.4235\n",
      "Epoch 3/5\n",
      "5868/5868 [==============================] - 19s - loss: 2.6007 - val_loss: 2.1166\n",
      "Epoch 4/5\n",
      "5868/5868 [==============================] - 20s - loss: 2.3222 - val_loss: 1.9712\n",
      "Epoch 5/5\n",
      "5868/5868 [==============================] - 19s - loss: 2.2104 - val_loss: 1.9375\n",
      "Training duration (s) :  115.56578612327576\n",
      "Train the 19.h5 model\n",
      "> Compilation Time :  0.026725053787231445\n",
      "Train on 5861 samples, validate on 652 samples\n",
      "Epoch 1/5\n",
      "5861/5861 [==============================] - 24s - loss: 3.8545 - val_loss: 2.9326\n",
      "Epoch 2/5\n",
      "5861/5861 [==============================] - 19s - loss: 3.0927 - val_loss: 2.5189\n",
      "Epoch 3/5\n",
      "5861/5861 [==============================] - 18s - loss: 2.7421 - val_loss: 2.2911\n",
      "Epoch 4/5\n",
      "5861/5861 [==============================] - 19s - loss: 2.5069 - val_loss: 2.1713\n",
      "Epoch 5/5\n",
      "5861/5861 [==============================] - 18s - loss: 2.3758 - val_loss: 2.1158\n",
      "Training duration (s) :  113.64167189598083\n",
      "Train the 20.h5 model\n",
      "> Compilation Time :  0.029586076736450195\n",
      "Train on 5847 samples, validate on 650 samples\n",
      "Epoch 1/5\n",
      "5847/5847 [==============================] - 22s - loss: 3.6477 - val_loss: 3.3297\n",
      "Epoch 2/5\n",
      "5847/5847 [==============================] - 18s - loss: 3.0224 - val_loss: 2.8687\n",
      "Epoch 3/5\n",
      "5847/5847 [==============================] - 19s - loss: 2.6804 - val_loss: 2.6088\n",
      "Epoch 4/5\n",
      "5847/5847 [==============================] - 18s - loss: 2.5049 - val_loss: 2.5261\n",
      "Epoch 5/5\n",
      "5847/5847 [==============================] - 19s - loss: 2.4253 - val_loss: 2.4825\n",
      "Training duration (s) :  112.06838130950928\n",
      "Train the 21.h5 model\n",
      "> Compilation Time :  0.027267932891845703\n",
      "Train on 5787 samples, validate on 643 samples\n",
      "Epoch 1/5\n",
      "5787/5787 [==============================] - 41s - loss: 3.2668 - val_loss: 1.9142\n",
      "Epoch 2/5\n",
      "5787/5787 [==============================] - 36s - loss: 2.4466 - val_loss: 1.5087\n",
      "Epoch 3/5\n",
      "5787/5787 [==============================] - 36s - loss: 2.1494 - val_loss: 1.3674\n",
      "Epoch 4/5\n",
      "5787/5787 [==============================] - 35s - loss: 2.0240 - val_loss: 1.3077\n",
      "Epoch 5/5\n",
      "5787/5787 [==============================] - 36s - loss: 1.9876 - val_loss: 1.2774\n",
      "Training duration (s) :  205.79637002944946\n",
      "Train the 22.h5 model\n",
      "> Compilation Time :  0.02679896354675293\n",
      "Train on 5780 samples, validate on 643 samples\n",
      "Epoch 1/5\n",
      "5780/5780 [==============================] - 41s - loss: 3.3144 - val_loss: 3.0460\n",
      "Epoch 2/5\n",
      "5780/5780 [==============================] - 36s - loss: 2.5357 - val_loss: 2.7464\n",
      "Epoch 3/5\n",
      "5780/5780 [==============================] - 37s - loss: 2.2469 - val_loss: 2.5484\n",
      "Epoch 4/5\n",
      "5780/5780 [==============================] - 36s - loss: 2.0960 - val_loss: 2.4218\n",
      "Epoch 5/5\n",
      "5780/5780 [==============================] - 38s - loss: 2.0276 - val_loss: 2.3449\n",
      "Training duration (s) :  208.5308928489685\n",
      "Train the 23.h5 model\n",
      "> Compilation Time :  0.026807785034179688\n",
      "Train on 5766 samples, validate on 641 samples\n",
      "Epoch 1/5\n",
      "5766/5766 [==============================] - 41s - loss: 3.8431 - val_loss: 2.6726\n",
      "Epoch 2/5\n",
      "5766/5766 [==============================] - 36s - loss: 3.1112 - val_loss: 2.2137\n",
      "Epoch 3/5\n",
      "5766/5766 [==============================] - 36s - loss: 2.7350 - val_loss: 2.0407\n",
      "Epoch 4/5\n",
      "5766/5766 [==============================] - 33s - loss: 2.6229 - val_loss: 1.9515\n",
      "Epoch 5/5\n",
      "5766/5766 [==============================] - 36s - loss: 2.5484 - val_loss: 1.9453\n",
      "Training duration (s) :  206.135746717453\n",
      "Train the 24.h5 model\n",
      "> Compilation Time :  0.028069257736206055\n",
      "Train on 5940 samples, validate on 661 samples\n",
      "Epoch 1/5\n",
      "5940/5940 [==============================] - 10s - loss: 29706.1199 - val_loss: 1.1895\n",
      "Epoch 2/5\n",
      "5940/5940 [==============================] - 6s - loss: 29704.1593 - val_loss: 1.0340\n",
      "Epoch 3/5\n",
      "5940/5940 [==============================] - 6s - loss: 29701.2612 - val_loss: 0.9680\n",
      "Epoch 4/5\n",
      "5940/5940 [==============================] - 6s - loss: 29697.7971 - val_loss: 0.9721\n",
      "Epoch 5/5\n",
      "5940/5940 [==============================] - 6s - loss: 29692.4846 - val_loss: 0.9040\n",
      "Training duration (s) :  44.43900799751282\n",
      "Train the 25.h5 model\n",
      "> Compilation Time :  0.02712702751159668\n",
      "Train on 5934 samples, validate on 660 samples\n",
      "Epoch 1/5\n",
      "5934/5934 [==============================] - 10s - loss: 28471.7189 - val_loss: 2.1671\n",
      "Epoch 2/5\n",
      "5934/5934 [==============================] - 6s - loss: 28467.9448 - val_loss: 1.9423\n",
      "Epoch 3/5\n",
      "5934/5934 [==============================] - 6s - loss: 28466.7754 - val_loss: 1.8618\n",
      "Epoch 4/5\n",
      "5934/5934 [==============================] - 6s - loss: 28461.4115 - val_loss: 1.7219\n",
      "Epoch 5/5\n",
      "5934/5934 [==============================] - 6s - loss: 28460.4871 - val_loss: 1.6428\n",
      "Training duration (s) :  45.08150911331177\n",
      "Train the 26.h5 model\n",
      "> Compilation Time :  0.027344703674316406\n",
      "Train on 5920 samples, validate on 658 samples\n",
      "Epoch 1/5\n",
      "5920/5920 [==============================] - 12s - loss: 30550.8860 - val_loss: 1.9891\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5920/5920 [==============================] - 6s - loss: 30549.1315 - val_loss: 1.7755\n",
      "Epoch 3/5\n",
      "5920/5920 [==============================] - 6s - loss: 30547.8614 - val_loss: 1.6497\n",
      "Epoch 4/5\n",
      "5920/5920 [==============================] - 6s - loss: 30545.3470 - val_loss: 1.5537\n",
      "Epoch 5/5\n",
      "5920/5920 [==============================] - 6s - loss: 30541.4406 - val_loss: 1.4738\n",
      "Training duration (s) :  47.62166094779968\n",
      "Train the 27.h5 model\n",
      "> Compilation Time :  0.030497074127197266\n",
      "Train on 5913 samples, validate on 658 samples\n",
      "Epoch 1/5\n",
      "5913/5913 [==============================] - 17s - loss: 18409.8535 - val_loss: 2.8671\n",
      "Epoch 2/5\n",
      "5913/5913 [==============================] - 12s - loss: 18406.7704 - val_loss: 2.3875\n",
      "Epoch 3/5\n",
      "5913/5913 [==============================] - 12s - loss: 18404.9486 - val_loss: 2.1693\n",
      "Epoch 4/5\n",
      "5913/5913 [==============================] - 12s - loss: 18401.0025 - val_loss: 2.0197\n",
      "Epoch 5/5\n",
      "5913/5913 [==============================] - 12s - loss: 18398.2534 - val_loss: 1.8692\n",
      "Training duration (s) :  78.86396813392639\n",
      "Train the 28.h5 model\n",
      "> Compilation Time :  0.026814937591552734\n",
      "Train on 5907 samples, validate on 657 samples\n",
      "Epoch 1/5\n",
      "5907/5907 [==============================] - 17s - loss: 23700.5430 - val_loss: 2.0446\n",
      "Epoch 2/5\n",
      "5907/5907 [==============================] - 12s - loss: 23697.7121 - val_loss: 1.9642\n",
      "Epoch 3/5\n",
      "5907/5907 [==============================] - 12s - loss: 23695.2743 - val_loss: 1.8601\n",
      "Epoch 4/5\n",
      "5907/5907 [==============================] - 12s - loss: 23691.7324 - val_loss: 1.7409\n",
      "Epoch 5/5\n",
      "5907/5907 [==============================] - 13s - loss: 23688.4031 - val_loss: 1.6793\n",
      "Training duration (s) :  80.35564184188843\n",
      "Train the 29.h5 model\n",
      "> Compilation Time :  0.02683424949645996\n",
      "Train on 5893 samples, validate on 655 samples\n",
      "Epoch 1/5\n",
      "5893/5893 [==============================] - 17s - loss: 3.5364 - val_loss: 245294.3089\n",
      "Epoch 2/5\n",
      "5893/5893 [==============================] - 11s - loss: 2.9316 - val_loss: 245259.7726\n",
      "Epoch 3/5\n",
      "5893/5893 [==============================] - 12s - loss: 2.5973 - val_loss: 245277.4582\n",
      "Epoch 4/5\n",
      "5893/5893 [==============================] - 12s - loss: 2.4167 - val_loss: 245273.8861\n",
      "Epoch 5/5\n",
      "5893/5893 [==============================] - 12s - loss: 2.2979 - val_loss: 245261.2743\n",
      "Training duration (s) :  81.10631489753723\n",
      "Train the 30.h5 model\n",
      "> Compilation Time :  0.027903079986572266\n",
      "Train on 5886 samples, validate on 655 samples\n",
      "Epoch 1/5\n",
      "5886/5886 [==============================] - 25s - loss: 112884.7759 - val_loss: 3.6489\n",
      "Epoch 2/5\n",
      "5886/5886 [==============================] - 19s - loss: 112878.3905 - val_loss: 3.1776\n",
      "Epoch 3/5\n",
      "5886/5886 [==============================] - 18s - loss: 112871.1237 - val_loss: 2.9370\n",
      "Epoch 4/5\n",
      "5886/5886 [==============================] - 19s - loss: 112864.3570 - val_loss: 2.6729\n",
      "Epoch 5/5\n",
      "5886/5886 [==============================] - 20s - loss: 112859.1119 - val_loss: 2.5140\n",
      "Training duration (s) :  118.09021282196045\n",
      "Train the 31.h5 model\n",
      "> Compilation Time :  0.036322832107543945\n",
      "Train on 5880 samples, validate on 654 samples\n",
      "Epoch 1/5\n",
      "5880/5880 [==============================] - 25s - loss: 84128.7214 - val_loss: 2.6069\n",
      "Epoch 2/5\n",
      "5880/5880 [==============================] - 19s - loss: 84123.9246 - val_loss: 2.3045\n",
      "Epoch 3/5\n",
      "5880/5880 [==============================] - 18s - loss: 84118.4980 - val_loss: 2.2340\n",
      "Epoch 4/5\n",
      "5880/5880 [==============================] - 18s - loss: 84112.2380 - val_loss: 2.0550\n",
      "Epoch 5/5\n",
      "5880/5880 [==============================] - 17s - loss: 84106.5499 - val_loss: 1.7461\n",
      "Training duration (s) :  116.87766671180725\n",
      "Train the 32.h5 model\n",
      "> Compilation Time :  0.026771068572998047\n",
      "Train on 5866 samples, validate on 652 samples\n",
      "Epoch 1/5\n",
      "5866/5866 [==============================] - 24s - loss: 108950.2060 - val_loss: 8.4391\n",
      "Epoch 2/5\n",
      "5866/5866 [==============================] - 18s - loss: 108946.6412 - val_loss: 7.9498\n",
      "Epoch 3/5\n",
      "5866/5866 [==============================] - 19s - loss: 108941.9805 - val_loss: 7.5529\n",
      "Epoch 4/5\n",
      "5866/5866 [==============================] - 18s - loss: 108938.0011 - val_loss: 7.2251\n",
      "Epoch 5/5\n",
      "5866/5866 [==============================] - 18s - loss: 108932.5016 - val_loss: 6.9217\n",
      "Training duration (s) :  115.56371402740479\n",
      "Train the 33.h5 model\n",
      "> Compilation Time :  0.0276491641998291\n",
      "Train on 5805 samples, validate on 646 samples\n",
      "Epoch 1/5\n",
      "5805/5805 [==============================] - 44s - loss: 18597.0694 - val_loss: 5.7577\n",
      "Epoch 2/5\n",
      "5805/5805 [==============================] - 37s - loss: 18592.3654 - val_loss: 5.3363\n",
      "Epoch 3/5\n",
      "5805/5805 [==============================] - 37s - loss: 18590.6921 - val_loss: 4.8682\n",
      "Epoch 4/5\n",
      "5805/5805 [==============================] - 37s - loss: 18587.6728 - val_loss: 4.2919\n",
      "Epoch 5/5\n",
      "5805/5805 [==============================] - 37s - loss: 18583.0344 - val_loss: 4.1024\n",
      "Training duration (s) :  213.88257312774658\n",
      "Train the 34.h5 model\n",
      "> Compilation Time :  0.03157997131347656\n",
      "Train on 5799 samples, validate on 645 samples\n",
      "Epoch 1/5\n",
      "5799/5799 [==============================] - 44s - loss: 17150.4224 - val_loss: 3.2828\n",
      "Epoch 2/5\n",
      "5799/5799 [==============================] - 34s - loss: 17146.5451 - val_loss: 3.0609\n",
      "Epoch 3/5\n",
      "5799/5799 [==============================] - 37s - loss: 17145.9436 - val_loss: 2.9862\n",
      "Epoch 4/5\n",
      "5799/5799 [==============================] - 38s - loss: 17140.5348 - val_loss: 2.6126\n",
      "Epoch 5/5\n",
      "5799/5799 [==============================] - 37s - loss: 17138.4211 - val_loss: 2.4511\n",
      "Training duration (s) :  215.52832508087158\n",
      "Train the 35.h5 model\n",
      "> Compilation Time :  0.026951074600219727\n",
      "Train on 5785 samples, validate on 643 samples\n",
      "Epoch 1/5\n",
      "5785/5785 [==============================] - 42s - loss: 7.9633 - val_loss: 302388.7731\n",
      "Epoch 2/5\n",
      "5785/5785 [==============================] - 36s - loss: 6.7030 - val_loss: 302391.4945\n",
      "Epoch 3/5\n",
      "5785/5785 [==============================] - 36s - loss: 5.8274 - val_loss: 302389.2749\n",
      "Epoch 4/5\n",
      "5785/5785 [==============================] - 36s - loss: 5.2481 - val_loss: 302380.3394\n",
      "Epoch 5/5\n",
      "5785/5785 [==============================] - 36s - loss: 4.8705 - val_loss: 302368.1677\n",
      "Training duration (s) :  212.55510592460632\n",
      "Train the 36.h5 model\n",
      "> Compilation Time :  0.02730083465576172\n",
      "Train on 5288 samples, validate on 588 samples\n",
      "Epoch 1/5\n",
      "5288/5288 [==============================] - 11s - loss: 405.3433 - val_loss: 1.1102\n",
      "Epoch 2/5\n",
      "5288/5288 [==============================] - 5s - loss: 403.7482 - val_loss: 1.0398\n",
      "Epoch 3/5\n",
      "5288/5288 [==============================] - 5s - loss: 402.4030 - val_loss: 0.8919\n",
      "Epoch 4/5\n",
      "5288/5288 [==============================] - 5s - loss: 400.9207 - val_loss: 0.8035\n",
      "Epoch 5/5\n",
      "5288/5288 [==============================] - 5s - loss: 399.3512 - val_loss: 0.7354\n",
      "Training duration (s) :  42.92740988731384\n",
      "Train the 37.h5 model\n",
      "> Compilation Time :  0.027209997177124023\n",
      "Train on 5282 samples, validate on 587 samples\n",
      "Epoch 1/5\n",
      "5282/5282 [==============================] - 12s - loss: 354.8365 - val_loss: 1.0248\n",
      "Epoch 2/5\n",
      "5282/5282 [==============================] - 5s - loss: 353.5289 - val_loss: 0.9141\n",
      "Epoch 3/5\n",
      "5282/5282 [==============================] - 5s - loss: 352.9496 - val_loss: 0.8110\n",
      "Epoch 4/5\n",
      "5282/5282 [==============================] - 5s - loss: 351.6284 - val_loss: 0.7322\n",
      "Epoch 5/5\n",
      "5282/5282 [==============================] - 5s - loss: 350.4515 - val_loss: 0.7041\n",
      "Training duration (s) :  43.806440114974976\n",
      "Train the 38.h5 model\n",
      "> Compilation Time :  0.02703404426574707\n",
      "Train on 5267 samples, validate on 586 samples\n",
      "Epoch 1/5\n",
      "5267/5267 [==============================] - 12s - loss: 206.6723 - val_loss: 1.3208\n",
      "Epoch 2/5\n",
      "5267/5267 [==============================] - 5s - loss: 206.0137 - val_loss: 1.1530\n",
      "Epoch 3/5\n",
      "5267/5267 [==============================] - 5s - loss: 205.2130 - val_loss: 1.0393\n",
      "Epoch 4/5\n",
      "5267/5267 [==============================] - 5s - loss: 204.5643 - val_loss: 0.9893\n",
      "Epoch 5/5\n",
      "5267/5267 [==============================] - 5s - loss: 203.5100 - val_loss: 0.9608\n",
      "Training duration (s) :  47.06305503845215\n",
      "Train the 39.h5 model\n",
      "> Compilation Time :  0.027579784393310547\n",
      "Train on 5261 samples, validate on 585 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5261/5261 [==============================] - 18s - loss: 129.3093 - val_loss: 0.9856\n",
      "Epoch 2/5\n",
      "5261/5261 [==============================] - 11s - loss: 128.0811 - val_loss: 0.9026\n",
      "Epoch 3/5\n",
      "5261/5261 [==============================] - 11s - loss: 127.4417 - val_loss: 0.8349\n",
      "Epoch 4/5\n",
      "5261/5261 [==============================] - 11s - loss: 126.3132 - val_loss: 0.8060\n",
      "Epoch 5/5\n",
      "5261/5261 [==============================] - 11s - loss: 125.7306 - val_loss: 0.7976\n",
      "Training duration (s) :  74.38375282287598\n",
      "Train the 40.h5 model\n",
      "> Compilation Time :  0.026966094970703125\n",
      "Train on 5255 samples, validate on 584 samples\n",
      "Epoch 1/5\n",
      "5255/5255 [==============================] - 18s - loss: 140.3287 - val_loss: 531.1328\n",
      "Epoch 2/5\n",
      "5255/5255 [==============================] - 10s - loss: 139.5813 - val_loss: 529.1493\n",
      "Epoch 3/5\n",
      "5255/5255 [==============================] - 11s - loss: 138.8809 - val_loss: 527.1718\n",
      "Epoch 4/5\n",
      "5255/5255 [==============================] - 11s - loss: 138.1837 - val_loss: 524.7307\n",
      "Epoch 5/5\n",
      "5255/5255 [==============================] - 11s - loss: 137.4892 - val_loss: 523.4176\n",
      "Training duration (s) :  74.11081790924072\n",
      "Train the 41.h5 model\n",
      "> Compilation Time :  0.031052827835083008\n",
      "Train on 5240 samples, validate on 583 samples\n",
      "Epoch 1/5\n",
      "5240/5240 [==============================] - 18s - loss: 147.1075 - val_loss: 602.2593\n",
      "Epoch 2/5\n",
      "5240/5240 [==============================] - 11s - loss: 146.3685 - val_loss: 600.2491\n",
      "Epoch 3/5\n",
      "5240/5240 [==============================] - 11s - loss: 145.7402 - val_loss: 598.1078\n",
      "Epoch 4/5\n",
      "5240/5240 [==============================] - 11s - loss: 144.9391 - val_loss: 595.7248\n",
      "Epoch 5/5\n",
      "5240/5240 [==============================] - 11s - loss: 143.8909 - val_loss: 594.1254\n",
      "Training duration (s) :  79.20429015159607\n",
      "Train the 42.h5 model\n",
      "> Compilation Time :  0.02713608741760254\n",
      "Train on 5234 samples, validate on 582 samples\n",
      "Epoch 1/5\n",
      "5234/5234 [==============================] - 24s - loss: 201.6632 - val_loss: 2.2898\n",
      "Epoch 2/5\n",
      "5234/5234 [==============================] - 17s - loss: 199.8820 - val_loss: 1.9497\n",
      "Epoch 3/5\n",
      "5234/5234 [==============================] - 17s - loss: 198.4081 - val_loss: 1.7121\n",
      "Epoch 4/5\n",
      "5234/5234 [==============================] - 16s - loss: 197.2472 - val_loss: 1.5630\n",
      "Epoch 5/5\n",
      "5234/5234 [==============================] - 17s - loss: 196.3562 - val_loss: 1.5118\n",
      "Training duration (s) :  105.24180698394775\n",
      "Train the 43.h5 model\n",
      "> Compilation Time :  0.02795124053955078\n",
      "Train on 5228 samples, validate on 581 samples\n",
      "Epoch 1/5\n",
      "5228/5228 [==============================] - 24s - loss: 249.3534 - val_loss: 1.9049\n",
      "Epoch 2/5\n",
      "5228/5228 [==============================] - 17s - loss: 248.2901 - val_loss: 1.6433\n",
      "Epoch 3/5\n",
      "5228/5228 [==============================] - 17s - loss: 247.0868 - val_loss: 1.3740\n",
      "Epoch 4/5\n",
      "5228/5228 [==============================] - 16s - loss: 245.1256 - val_loss: 1.2213\n",
      "Epoch 5/5\n",
      "5228/5228 [==============================] - 17s - loss: 244.1794 - val_loss: 1.0906\n",
      "Training duration (s) :  106.14787602424622\n",
      "Train the 44.h5 model\n",
      "> Compilation Time :  0.03934192657470703\n",
      "Train on 5213 samples, validate on 580 samples\n",
      "Epoch 1/5\n",
      "5213/5213 [==============================] - 24s - loss: 217.6513 - val_loss: 38.6780\n",
      "Epoch 2/5\n",
      "5213/5213 [==============================] - 17s - loss: 216.5935 - val_loss: 37.7459\n",
      "Epoch 3/5\n",
      "5213/5213 [==============================] - 16s - loss: 215.7640 - val_loss: 36.8738\n",
      "Epoch 4/5\n",
      "5213/5213 [==============================] - 17s - loss: 214.7007 - val_loss: 35.9382\n",
      "Epoch 5/5\n",
      "5213/5213 [==============================] - 17s - loss: 213.4573 - val_loss: 35.1838\n",
      "Training duration (s) :  109.98278999328613\n",
      "Train the 45.h5 model\n",
      "> Compilation Time :  0.03967595100402832\n",
      "Train on 5153 samples, validate on 573 samples\n",
      "Epoch 1/5\n",
      "5153/5153 [==============================] - 42s - loss: 693.0350 - val_loss: 3.8877\n",
      "Epoch 2/5\n",
      "5153/5153 [==============================] - 35s - loss: 690.5358 - val_loss: 3.2773\n",
      "Epoch 3/5\n",
      "5153/5153 [==============================] - 32s - loss: 687.9339 - val_loss: 2.6366\n",
      "Epoch 4/5\n",
      "5153/5153 [==============================] - 33s - loss: 684.5938 - val_loss: 2.2196\n",
      "Epoch 5/5\n",
      "5153/5153 [==============================] - 33s - loss: 683.4857 - val_loss: 1.9750\n",
      "Training duration (s) :  197.3538682460785\n",
      "Train the 46.h5 model\n",
      "> Compilation Time :  0.027151823043823242\n",
      "Train on 5147 samples, validate on 572 samples\n",
      "Epoch 1/5\n",
      "5147/5147 [==============================] - 39s - loss: 907.8141 - val_loss: 8.4354\n",
      "Epoch 2/5\n",
      "5147/5147 [==============================] - 33s - loss: 905.3324 - val_loss: 7.6665\n",
      "Epoch 3/5\n",
      "5147/5147 [==============================] - 34s - loss: 903.1985 - val_loss: 6.8694\n",
      "Epoch 4/5\n",
      "5147/5147 [==============================] - 33s - loss: 901.2095 - val_loss: 6.2078\n",
      "Epoch 5/5\n",
      "5147/5147 [==============================] - 33s - loss: 898.4415 - val_loss: 5.7843\n",
      "Training duration (s) :  193.18292880058289\n",
      "Train the 47.h5 model\n",
      "> Compilation Time :  0.02662801742553711\n",
      "Train on 5132 samples, validate on 571 samples\n",
      "Epoch 1/5\n",
      "5132/5132 [==============================] - 48s - loss: 1181.2982 - val_loss: 4.2313\n",
      "Epoch 2/5\n",
      "5132/5132 [==============================] - 34s - loss: 1178.9256 - val_loss: 3.8135\n",
      "Epoch 3/5\n",
      "5132/5132 [==============================] - 34s - loss: 1176.9466 - val_loss: 3.1374\n",
      "Epoch 4/5\n",
      "5132/5132 [==============================] - 34s - loss: 1173.7266 - val_loss: 2.7197\n",
      "Epoch 5/5\n",
      "5132/5132 [==============================] - 35s - loss: 1171.0731 - val_loss: 2.4742\n",
      "Training duration (s) :  207.02025723457336\n",
      "Train the 48.h5 model\n",
      "> Compilation Time :  0.03680109977722168\n",
      "Train on 5239 samples, validate on 583 samples\n",
      "Epoch 1/5\n",
      "5239/5239 [==============================] - 14s - loss: 1104.3518 - val_loss: 1.8734\n",
      "Epoch 2/5\n",
      "5239/5239 [==============================] - 5s - loss: 1102.7028 - val_loss: 1.7667\n",
      "Epoch 3/5\n",
      "5239/5239 [==============================] - 5s - loss: 1101.0187 - val_loss: 1.6037\n",
      "Epoch 4/5\n",
      "5239/5239 [==============================] - 5s - loss: 1099.4122 - val_loss: 1.5793\n",
      "Epoch 5/5\n",
      "5239/5239 [==============================] - 5s - loss: 1098.4276 - val_loss: 1.5002\n",
      "Training duration (s) :  47.7666962146759\n",
      "Train the 49.h5 model\n",
      "> Compilation Time :  0.028902053833007812\n",
      "Train on 5233 samples, validate on 582 samples\n",
      "Epoch 1/5\n",
      "5233/5233 [==============================] - 13s - loss: 906.3865 - val_loss: 42.4924\n",
      "Epoch 2/5\n",
      "5233/5233 [==============================] - 5s - loss: 904.8471 - val_loss: 41.9072\n",
      "Epoch 3/5\n",
      "5233/5233 [==============================] - 5s - loss: 903.3381 - val_loss: 41.3829\n",
      "Epoch 4/5\n",
      "5233/5233 [==============================] - 5s - loss: 902.4562 - val_loss: 40.7424\n",
      "Epoch 5/5\n",
      "5233/5233 [==============================] - 5s - loss: 900.9785 - val_loss: 40.3537\n",
      "Training duration (s) :  46.40442895889282\n",
      "Train the 50.h5 model\n",
      "> Compilation Time :  0.029300212860107422\n",
      "Train on 5219 samples, validate on 580 samples\n",
      "Epoch 1/5\n",
      "5219/5219 [==============================] - 13s - loss: 888.7465 - val_loss: 3.2117\n",
      "Epoch 2/5\n",
      "5219/5219 [==============================] - 5s - loss: 887.2074 - val_loss: 2.9067\n",
      "Epoch 3/5\n",
      "5219/5219 [==============================] - 5s - loss: 886.4489 - val_loss: 2.7485\n",
      "Epoch 4/5\n",
      "5219/5219 [==============================] - 5s - loss: 885.5664 - val_loss: 2.5902\n",
      "Epoch 5/5\n",
      "5219/5219 [==============================] - 5s - loss: 884.4530 - val_loss: 2.5281\n",
      "Training duration (s) :  47.31491303443909\n",
      "Train the 51.h5 model\n",
      "> Compilation Time :  0.027151107788085938\n",
      "Train on 5212 samples, validate on 580 samples\n",
      "Epoch 1/5\n",
      "5212/5212 [==============================] - 21s - loss: 832.1070 - val_loss: 13.9266\n",
      "Epoch 2/5\n",
      "5212/5212 [==============================] - 11s - loss: 830.2576 - val_loss: 13.3256\n",
      "Epoch 3/5\n",
      "5212/5212 [==============================] - 11s - loss: 829.1487 - val_loss: 12.6728\n",
      "Epoch 4/5\n",
      "5212/5212 [==============================] - 11s - loss: 827.6214 - val_loss: 12.1208\n",
      "Epoch 5/5\n",
      "5212/5212 [==============================] - 11s - loss: 827.0626 - val_loss: 11.6953\n",
      "Training duration (s) :  78.44354104995728\n",
      "Train the 52.h5 model\n",
      "> Compilation Time :  0.027448177337646484\n",
      "Train on 5206 samples, validate on 579 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5206/5206 [==============================] - 19s - loss: 1458.6034 - val_loss: 12.8835\n",
      "Epoch 2/5\n",
      "5206/5206 [==============================] - 11s - loss: 1457.1382 - val_loss: 12.4940\n",
      "Epoch 3/5\n",
      "5206/5206 [==============================] - 11s - loss: 1455.7518 - val_loss: 11.9738\n",
      "Epoch 4/5\n",
      "5206/5206 [==============================] - 11s - loss: 1453.7346 - val_loss: 11.5295\n",
      "Epoch 5/5\n",
      "5206/5206 [==============================] - 11s - loss: 1452.4364 - val_loss: 11.2559\n",
      "Training duration (s) :  77.89054894447327\n",
      "Train the 53.h5 model\n",
      "> Compilation Time :  0.02742600440979004\n",
      "Train on 5192 samples, validate on 577 samples\n",
      "Epoch 1/5\n",
      "5192/5192 [==============================] - 20s - loss: 895.7786 - val_loss: 6.8379\n",
      "Epoch 2/5\n",
      "5192/5192 [==============================] - 11s - loss: 894.5290 - val_loss: 6.2999\n",
      "Epoch 3/5\n",
      "5192/5192 [==============================] - 13s - loss: 893.3359 - val_loss: 5.8429\n",
      "Epoch 4/5\n",
      "5192/5192 [==============================] - 11s - loss: 892.4975 - val_loss: 5.5522\n",
      "Epoch 5/5\n",
      "5192/5192 [==============================] - 10s - loss: 891.3654 - val_loss: 5.4280\n",
      "Training duration (s) :  80.88516402244568\n",
      "Train the 54.h5 model\n",
      "> Compilation Time :  0.028290987014770508\n",
      "Train on 5185 samples, validate on 577 samples\n",
      "Epoch 1/5\n",
      "5185/5185 [==============================] - 26s - loss: 540.2688 - val_loss: 3.8670\n",
      "Epoch 2/5\n",
      "5185/5185 [==============================] - 17s - loss: 538.5780 - val_loss: 3.2869\n",
      "Epoch 3/5\n",
      "5185/5185 [==============================] - 17s - loss: 537.1927 - val_loss: 2.7808\n",
      "Epoch 4/5\n",
      "5185/5185 [==============================] - 17s - loss: 535.6946 - val_loss: 2.5077\n",
      "Epoch 5/5\n",
      "5185/5185 [==============================] - 17s - loss: 533.7965 - val_loss: 2.2828\n",
      "Training duration (s) :  108.29344892501831\n",
      "Train the 55.h5 model\n",
      "> Compilation Time :  0.026770830154418945\n",
      "Train on 5179 samples, validate on 576 samples\n",
      "Epoch 1/5\n",
      "5179/5179 [==============================] - 26s - loss: 509.7229 - val_loss: 21.4544\n",
      "Epoch 2/5\n",
      "5179/5179 [==============================] - 17s - loss: 508.2365 - val_loss: 20.4118\n",
      "Epoch 3/5\n",
      "5179/5179 [==============================] - 17s - loss: 507.0546 - val_loss: 19.2500\n",
      "Epoch 4/5\n",
      "5179/5179 [==============================] - 17s - loss: 505.7706 - val_loss: 18.3964\n",
      "Epoch 5/5\n",
      "5179/5179 [==============================] - 17s - loss: 504.8662 - val_loss: 17.7760\n",
      "Training duration (s) :  111.4983959197998\n",
      "Train the 56.h5 model\n",
      "> Compilation Time :  0.03246307373046875\n",
      "Train on 5165 samples, validate on 574 samples\n",
      "Epoch 1/5\n",
      "5165/5165 [==============================] - 26s - loss: 688.5268 - val_loss: 15.6013\n",
      "Epoch 2/5\n",
      "5165/5165 [==============================] - 17s - loss: 687.1338 - val_loss: 14.8370\n",
      "Epoch 3/5\n",
      "5165/5165 [==============================] - 17s - loss: 686.2611 - val_loss: 14.1292\n",
      "Epoch 4/5\n",
      "5165/5165 [==============================] - 17s - loss: 684.9350 - val_loss: 13.4340\n",
      "Epoch 5/5\n",
      "5165/5165 [==============================] - 17s - loss: 683.8860 - val_loss: 13.0359\n",
      "Training duration (s) :  111.41912007331848\n",
      "Train the 57.h5 model\n",
      "> Compilation Time :  0.028175830841064453\n",
      "Train on 5104 samples, validate on 568 samples\n",
      "Epoch 1/5\n",
      "5104/5104 [==============================] - 42s - loss: 308.9823 - val_loss: 9.7920\n",
      "Epoch 2/5\n",
      "5104/5104 [==============================] - 33s - loss: 306.1894 - val_loss: 8.8498\n",
      "Epoch 3/5\n",
      "5104/5104 [==============================] - 32s - loss: 304.6365 - val_loss: 7.7199\n",
      "Epoch 4/5\n",
      "5104/5104 [==============================] - 32s - loss: 303.3851 - val_loss: 7.2683\n",
      "Epoch 5/5\n",
      "5104/5104 [==============================] - 32s - loss: 301.4383 - val_loss: 6.6934\n",
      "Training duration (s) :  192.9998881816864\n",
      "Train the 58.h5 model\n",
      "> Compilation Time :  0.027122974395751953\n",
      "Train on 5098 samples, validate on 567 samples\n",
      "Epoch 1/5\n",
      "5098/5098 [==============================] - 41s - loss: 225.9950 - val_loss: 63.8955\n",
      "Epoch 2/5\n",
      "5098/5098 [==============================] - 32s - loss: 223.7841 - val_loss: 62.2316\n",
      "Epoch 3/5\n",
      "5098/5098 [==============================] - 33s - loss: 222.6240 - val_loss: 60.2983\n",
      "Epoch 4/5\n",
      "5098/5098 [==============================] - 32s - loss: 220.9303 - val_loss: 58.8016\n",
      "Epoch 5/5\n",
      "5098/5098 [==============================] - 31s - loss: 219.7503 - val_loss: 57.6864\n",
      "Training duration (s) :  191.54295825958252\n",
      "Train the 59.h5 model\n",
      "> Compilation Time :  0.0302278995513916\n",
      "Train on 5084 samples, validate on 565 samples\n",
      "Epoch 1/5\n",
      "5084/5084 [==============================] - 42s - loss: 709.2412 - val_loss: 90.1096\n",
      "Epoch 2/5\n",
      "5084/5084 [==============================] - 32s - loss: 707.6603 - val_loss: 88.4788\n",
      "Epoch 3/5\n",
      "5084/5084 [==============================] - 32s - loss: 705.8224 - val_loss: 86.6840\n",
      "Epoch 4/5\n",
      "5084/5084 [==============================] - 32s - loss: 703.6157 - val_loss: 85.1873\n",
      "Epoch 5/5\n",
      "5084/5084 [==============================] - 33s - loss: 702.1666 - val_loss: 83.9814\n",
      "Training duration (s) :  194.5588037967682\n",
      "Train the 60.h5 model\n",
      "> Compilation Time :  0.033123016357421875\n",
      "Train on 3767 samples, validate on 419 samples\n",
      "Epoch 1/5\n",
      "3767/3767 [==============================] - 13s - loss: 11.3563 - val_loss: 24.5284\n",
      "Epoch 2/5\n",
      "3767/3767 [==============================] - 3s - loss: 10.7784 - val_loss: 23.6018\n",
      "Epoch 3/5\n",
      "3767/3767 [==============================] - 4s - loss: 10.3433 - val_loss: 22.8256\n",
      "Epoch 4/5\n",
      "3767/3767 [==============================] - 4s - loss: 9.9594 - val_loss: 21.8897\n",
      "Epoch 5/5\n",
      "3767/3767 [==============================] - 4s - loss: 9.4810 - val_loss: 21.1440\n",
      "Training duration (s) :  39.726945877075195\n",
      "Train the 61.h5 model\n",
      "> Compilation Time :  0.035295963287353516\n",
      "Train on 3761 samples, validate on 418 samples\n",
      "Epoch 1/5\n",
      "3761/3761 [==============================] - 13s - loss: 13.0985 - val_loss: 38.1625\n",
      "Epoch 2/5\n",
      "3761/3761 [==============================] - 4s - loss: 12.6519 - val_loss: 37.2687\n",
      "Epoch 3/5\n",
      "3761/3761 [==============================] - 4s - loss: 12.2163 - val_loss: 36.6186\n",
      "Epoch 4/5\n",
      "3761/3761 [==============================] - 3s - loss: 11.8923 - val_loss: 35.6866\n",
      "Epoch 5/5\n",
      "3761/3761 [==============================] - 4s - loss: 11.4168 - val_loss: 34.8338\n",
      "Training duration (s) :  40.08122706413269\n",
      "Train the 62.h5 model\n",
      "> Compilation Time :  0.02776503562927246\n",
      "Train on 3746 samples, validate on 417 samples\n",
      "Epoch 1/5\n",
      "3746/3746 [==============================] - 14s - loss: 20.0335 - val_loss: 8.5108\n",
      "Epoch 2/5\n",
      "3746/3746 [==============================] - 4s - loss: 19.5429 - val_loss: 8.0385\n",
      "Epoch 3/5\n",
      "3746/3746 [==============================] - 4s - loss: 19.1307 - val_loss: 7.6456\n",
      "Epoch 4/5\n",
      "3746/3746 [==============================] - 3s - loss: 18.6579 - val_loss: 7.1724\n",
      "Epoch 5/5\n",
      "3746/3746 [==============================] - 4s - loss: 18.1538 - val_loss: 6.7573\n",
      "Training duration (s) :  40.81992292404175\n",
      "Train the 63.h5 model\n",
      "> Compilation Time :  0.02732372283935547\n",
      "Train on 3740 samples, validate on 416 samples\n",
      "Epoch 1/5\n",
      "3740/3740 [==============================] - 18s - loss: 29.2832 - val_loss: 6.8292\n",
      "Epoch 2/5\n",
      "3740/3740 [==============================] - 8s - loss: 28.0976 - val_loss: 6.1865\n",
      "Epoch 3/5\n",
      "3740/3740 [==============================] - 8s - loss: 27.1419 - val_loss: 5.5763\n",
      "Epoch 4/5\n",
      "3740/3740 [==============================] - 8s - loss: 26.1259 - val_loss: 5.0335\n",
      "Epoch 5/5\n",
      "3740/3740 [==============================] - 8s - loss: 25.1543 - val_loss: 4.6845\n",
      "Training duration (s) :  61.91535782814026\n",
      "Train the 64.h5 model\n",
      "> Compilation Time :  0.027184724807739258\n",
      "Train on 3734 samples, validate on 415 samples\n",
      "Epoch 1/5\n",
      "3734/3734 [==============================] - 20s - loss: 19.6081 - val_loss: 106.0188\n",
      "Epoch 2/5\n",
      "3734/3734 [==============================] - 8s - loss: 18.8295 - val_loss: 103.7301\n",
      "Epoch 3/5\n",
      "3734/3734 [==============================] - 8s - loss: 18.0025 - val_loss: 102.1783\n",
      "Epoch 4/5\n",
      "3734/3734 [==============================] - 8s - loss: 17.2846 - val_loss: 99.6205\n",
      "Epoch 5/5\n",
      "3734/3734 [==============================] - 8s - loss: 16.4309 - val_loss: 98.1766\n",
      "Training duration (s) :  69.15363502502441\n",
      "Train the 65.h5 model\n",
      "> Compilation Time :  0.02694416046142578\n",
      "Train on 3719 samples, validate on 414 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3719/3719 [==============================] - 19s - loss: 42.1735 - val_loss: 13.0608\n",
      "Epoch 2/5\n",
      "3719/3719 [==============================] - 8s - loss: 41.3290 - val_loss: 12.4379\n",
      "Epoch 3/5\n",
      "3719/3719 [==============================] - 8s - loss: 40.3870 - val_loss: 11.6463\n",
      "Epoch 4/5\n",
      "3719/3719 [==============================] - 8s - loss: 39.4751 - val_loss: 10.8419\n",
      "Epoch 5/5\n",
      "3719/3719 [==============================] - 9s - loss: 38.3208 - val_loss: 10.3263\n",
      "Training duration (s) :  67.43928122520447\n",
      "Train the 66.h5 model\n",
      "> Compilation Time :  0.030579805374145508\n",
      "Train on 3713 samples, validate on 413 samples\n",
      "Epoch 1/5\n",
      "3713/3713 [==============================] - 24s - loss: 52.6222 - val_loss: 34.8560\n",
      "Epoch 2/5\n",
      "3713/3713 [==============================] - 12s - loss: 50.8970 - val_loss: 33.1686\n",
      "Epoch 3/5\n",
      "3713/3713 [==============================] - 11s - loss: 49.6609 - val_loss: 31.3869\n",
      "Epoch 4/5\n",
      "3713/3713 [==============================] - 12s - loss: 48.3426 - val_loss: 29.6196\n",
      "Epoch 5/5\n",
      "3713/3713 [==============================] - 12s - loss: 46.7555 - val_loss: 28.4078\n",
      "Training duration (s) :  86.89039897918701\n",
      "Train the 67.h5 model\n",
      "> Compilation Time :  0.02710413932800293\n",
      "Train on 3707 samples, validate on 412 samples\n",
      "Epoch 1/5\n",
      "3707/3707 [==============================] - 23s - loss: 64.6987 - val_loss: 10.1203\n",
      "Epoch 2/5\n",
      "3707/3707 [==============================] - 12s - loss: 63.5055 - val_loss: 9.4829\n",
      "Epoch 3/5\n",
      "3707/3707 [==============================] - 12s - loss: 62.2765 - val_loss: 8.8018\n",
      "Epoch 4/5\n",
      "3707/3707 [==============================] - 13s - loss: 60.7556 - val_loss: 8.1095\n",
      "Epoch 5/5\n",
      "3707/3707 [==============================] - 13s - loss: 59.2862 - val_loss: 7.5512\n",
      "Training duration (s) :  87.79697489738464\n",
      "Train the 68.h5 model\n",
      "> Compilation Time :  0.02697896957397461\n",
      "Train on 3692 samples, validate on 411 samples\n",
      "Epoch 1/5\n",
      "3692/3692 [==============================] - 23s - loss: 62.4625 - val_loss: 26.8606\n",
      "Epoch 2/5\n",
      "3692/3692 [==============================] - 13s - loss: 61.4749 - val_loss: 25.4082\n",
      "Epoch 3/5\n",
      "3692/3692 [==============================] - 12s - loss: 60.2720 - val_loss: 23.7355\n",
      "Epoch 4/5\n",
      "3692/3692 [==============================] - 12s - loss: 58.8993 - val_loss: 21.9965\n",
      "Epoch 5/5\n",
      "3692/3692 [==============================] - 12s - loss: 57.5845 - val_loss: 20.7908\n",
      "Training duration (s) :  87.72467994689941\n",
      "Train the 69.h5 model\n",
      "> Compilation Time :  0.02767181396484375\n",
      "Train on 3632 samples, validate on 404 samples\n",
      "Epoch 1/5\n",
      "3632/3632 [==============================] - 34s - loss: 48.8902 - val_loss: 5.6992\n",
      "Epoch 2/5\n",
      "3632/3632 [==============================] - 23s - loss: 47.1850 - val_loss: 5.1416\n",
      "Epoch 3/5\n",
      "3632/3632 [==============================] - 24s - loss: 45.8186 - val_loss: 4.5093\n",
      "Epoch 4/5\n",
      "3632/3632 [==============================] - 23s - loss: 44.2565 - val_loss: 4.1443\n",
      "Epoch 5/5\n",
      "3632/3632 [==============================] - 23s - loss: 42.6834 - val_loss: 3.7183\n",
      "Training duration (s) :  147.23443365097046\n",
      "Train the 70.h5 model\n",
      "> Compilation Time :  0.03084421157836914\n",
      "Train on 3626 samples, validate on 403 samples\n",
      "Epoch 1/5\n",
      "3626/3626 [==============================] - 35s - loss: 50.4642 - val_loss: 2.7606\n",
      "Epoch 2/5\n",
      "3626/3626 [==============================] - 23s - loss: 49.0204 - val_loss: 2.6541\n",
      "Epoch 3/5\n",
      "3626/3626 [==============================] - 23s - loss: 47.8157 - val_loss: 2.0949\n",
      "Epoch 4/5\n",
      "3626/3626 [==============================] - 24s - loss: 46.6123 - val_loss: 1.9419\n",
      "Epoch 5/5\n",
      "3626/3626 [==============================] - 23s - loss: 45.4353 - val_loss: 1.7519\n",
      "Training duration (s) :  146.95565795898438\n",
      "Train the 71.h5 model\n",
      "> Compilation Time :  0.028208017349243164\n",
      "Train on 3611 samples, validate on 402 samples\n",
      "Epoch 1/5\n",
      "3611/3611 [==============================] - 36s - loss: 62.5704 - val_loss: 11.0643\n",
      "Epoch 2/5\n",
      "3611/3611 [==============================] - 23s - loss: 61.2688 - val_loss: 10.1541\n",
      "Epoch 3/5\n",
      "3611/3611 [==============================] - 23s - loss: 59.9336 - val_loss: 9.1855\n",
      "Epoch 4/5\n",
      "3611/3611 [==============================] - 23s - loss: 58.3497 - val_loss: 8.1846\n",
      "Epoch 5/5\n",
      "3611/3611 [==============================] - 22s - loss: 56.8026 - val_loss: 7.6393\n",
      "Training duration (s) :  146.96154618263245\n",
      "Train the 72.h5 model\n",
      "> Compilation Time :  0.027842998504638672\n",
      "Train on 3415 samples, validate on 380 samples\n",
      "Epoch 1/5\n",
      "3415/3415 [==============================] - 17s - loss: 87.5594 - val_loss: 1.2135\n",
      "Epoch 2/5\n",
      "3415/3415 [==============================] - 3s - loss: 86.8923 - val_loss: 0.9903\n",
      "Epoch 3/5\n",
      "3415/3415 [==============================] - 3s - loss: 86.2451 - val_loss: 0.8765\n",
      "Epoch 4/5\n",
      "3415/3415 [==============================] - 3s - loss: 85.6508 - val_loss: 0.8870\n",
      "Epoch 5/5\n",
      "3415/3415 [==============================] - 3s - loss: 84.9749 - val_loss: 0.8570\n",
      "Training duration (s) :  43.07224702835083\n",
      "Train the 73.h5 model\n",
      "> Compilation Time :  0.037236928939819336\n",
      "Train on 3409 samples, validate on 379 samples\n",
      "Epoch 1/5\n",
      "3409/3409 [==============================] - 16s - loss: 202.0420 - val_loss: 13.5708\n",
      "Epoch 2/5\n",
      "3409/3409 [==============================] - 3s - loss: 201.5987 - val_loss: 13.1418\n",
      "Epoch 3/5\n",
      "3409/3409 [==============================] - 3s - loss: 200.9271 - val_loss: 12.9435\n",
      "Epoch 4/5\n",
      "3409/3409 [==============================] - 3s - loss: 200.2630 - val_loss: 12.6867\n",
      "Epoch 5/5\n",
      "3409/3409 [==============================] - 3s - loss: 199.4894 - val_loss: 12.4660\n",
      "Training duration (s) :  43.94261693954468\n",
      "Train the 74.h5 model\n",
      "> Compilation Time :  0.030987977981567383\n",
      "Train on 3394 samples, validate on 378 samples\n",
      "Epoch 1/5\n",
      "3394/3394 [==============================] - 17s - loss: 104.2256 - val_loss: 2.3768\n",
      "Epoch 2/5\n",
      "3394/3394 [==============================] - 4s - loss: 103.7040 - val_loss: 2.1016\n",
      "Epoch 3/5\n",
      "3394/3394 [==============================] - 3s - loss: 103.2501 - val_loss: 2.0149\n",
      "Epoch 4/5\n",
      "3394/3394 [==============================] - 3s - loss: 102.7069 - val_loss: 1.8686\n",
      "Epoch 5/5\n",
      "3394/3394 [==============================] - 3s - loss: 102.3066 - val_loss: 1.7984\n",
      "Training duration (s) :  45.453230142593384\n",
      "Train the 75.h5 model\n",
      "> Compilation Time :  0.03658914566040039\n",
      "Train on 3388 samples, validate on 377 samples\n",
      "Epoch 1/5\n",
      "3388/3388 [==============================] - 20s - loss: 7.6970 - val_loss: 1.6291\n",
      "Epoch 2/5\n",
      "3388/3388 [==============================] - 8s - loss: 7.0291 - val_loss: 1.4173\n",
      "Epoch 3/5\n",
      "3388/3388 [==============================] - 7s - loss: 6.6575 - val_loss: 1.2351\n",
      "Epoch 4/5\n",
      "3388/3388 [==============================] - 7s - loss: 6.3256 - val_loss: 1.2190\n",
      "Epoch 5/5\n",
      "3388/3388 [==============================] - 7s - loss: 5.9913 - val_loss: 1.1400\n",
      "Training duration (s) :  70.67468214035034\n",
      "Train the 76.h5 model\n",
      "> Compilation Time :  0.029368162155151367\n",
      "Train on 3382 samples, validate on 376 samples\n",
      "Epoch 1/5\n",
      "3382/3382 [==============================] - 22s - loss: 20.8082 - val_loss: 3.2231\n",
      "Epoch 2/5\n",
      "3382/3382 [==============================] - 7s - loss: 20.0967 - val_loss: 2.8741\n",
      "Epoch 3/5\n",
      "3382/3382 [==============================] - 7s - loss: 19.7186 - val_loss: 2.5690\n",
      "Epoch 4/5\n",
      "3382/3382 [==============================] - 7s - loss: 19.3627 - val_loss: 2.4028\n",
      "Epoch 5/5\n",
      "3382/3382 [==============================] - 7s - loss: 18.9942 - val_loss: 2.1487\n",
      "Training duration (s) :  64.49187397956848\n",
      "Train the 77.h5 model\n",
      "> Compilation Time :  0.03375387191772461\n",
      "Train on 3367 samples, validate on 375 samples\n",
      "Epoch 1/5\n",
      "3367/3367 [==============================] - 19s - loss: 34.0209 - val_loss: 2.1565\n",
      "Epoch 2/5\n",
      "3367/3367 [==============================] - 7s - loss: 33.4471 - val_loss: 1.9235\n",
      "Epoch 3/5\n",
      "3367/3367 [==============================] - 7s - loss: 32.9770 - val_loss: 1.6580\n",
      "Epoch 4/5\n",
      "3367/3367 [==============================] - 7s - loss: 32.5090 - val_loss: 1.5763\n",
      "Epoch 5/5\n",
      "3367/3367 [==============================] - 7s - loss: 32.0306 - val_loss: 1.4693\n",
      "Training duration (s) :  61.19177770614624\n",
      "Train the 78.h5 model\n",
      "> Compilation Time :  0.027502059936523438\n",
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3361/3361 [==============================] - 25s - loss: 11.7966 - val_loss: 4.4256\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 11s - loss: 11.0019 - val_loss: 3.9958\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 11s - loss: 10.4144 - val_loss: 3.5639\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 12s - loss: 10.0298 - val_loss: 3.2374\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 11s - loss: 9.5348 - val_loss: 3.0238\n",
      "Training duration (s) :  85.3069589138031\n",
      "Train the 79.h5 model\n",
      "> Compilation Time :  0.027675151824951172\n",
      "Train on 3355 samples, validate on 373 samples\n",
      "Epoch 1/5\n",
      "3355/3355 [==============================] - 23s - loss: 20.7684 - val_loss: 2.1504\n",
      "Epoch 2/5\n",
      "3355/3355 [==============================] - 12s - loss: 20.1443 - val_loss: 1.7583\n",
      "Epoch 3/5\n",
      "3355/3355 [==============================] - 11s - loss: 19.5306 - val_loss: 1.5633\n",
      "Epoch 4/5\n",
      "3355/3355 [==============================] - 11s - loss: 19.0593 - val_loss: 1.4359\n",
      "Epoch 5/5\n",
      "3355/3355 [==============================] - 11s - loss: 18.5843 - val_loss: 1.3083\n",
      "Training duration (s) :  82.58098888397217\n",
      "Train the 80.h5 model\n",
      "> Compilation Time :  0.029902935028076172\n",
      "Train on 3340 samples, validate on 372 samples\n",
      "Epoch 1/5\n",
      "3340/3340 [==============================] - 22s - loss: 32.8815 - val_loss: 97.7526\n",
      "Epoch 2/5\n",
      "3340/3340 [==============================] - 10s - loss: 32.3266 - val_loss: 96.7512\n",
      "Epoch 3/5\n",
      "3340/3340 [==============================] - 11s - loss: 31.8346 - val_loss: 95.9607\n",
      "Epoch 4/5\n",
      "3340/3340 [==============================] - 10s - loss: 31.3445 - val_loss: 95.0250\n",
      "Epoch 5/5\n",
      "3340/3340 [==============================] - 10s - loss: 30.9272 - val_loss: 94.1480\n",
      "Training duration (s) :  80.59783506393433\n",
      "Train the 81.h5 model\n",
      "> Compilation Time :  0.027740955352783203\n",
      "Train on 3280 samples, validate on 365 samples\n",
      "Epoch 1/5\n",
      "3280/3280 [==============================] - 35s - loss: 207.6032 - val_loss: 714.1759\n",
      "Epoch 2/5\n",
      "3280/3280 [==============================] - 21s - loss: 206.2832 - val_loss: 711.5426\n",
      "Epoch 3/5\n",
      "3280/3280 [==============================] - 21s - loss: 205.2860 - val_loss: 709.1506\n",
      "Epoch 4/5\n",
      "3280/3280 [==============================] - 21s - loss: 204.3458 - val_loss: 706.9579\n",
      "Epoch 5/5\n",
      "3280/3280 [==============================] - 21s - loss: 203.4848 - val_loss: 704.4382\n",
      "Training duration (s) :  138.39956283569336\n",
      "Train the 82.h5 model\n",
      "> Compilation Time :  0.027811050415039062\n",
      "Train on 3274 samples, validate on 364 samples\n",
      "Epoch 1/5\n",
      "3274/3274 [==============================] - 33s - loss: 173.6102 - val_loss: 500.1508\n",
      "Epoch 2/5\n",
      "3274/3274 [==============================] - 21s - loss: 172.7222 - val_loss: 498.0108\n",
      "Epoch 3/5\n",
      "3274/3274 [==============================] - 21s - loss: 171.9079 - val_loss: 496.4587\n",
      "Epoch 4/5\n",
      "3274/3274 [==============================] - 21s - loss: 171.2178 - val_loss: 495.1187\n",
      "Epoch 5/5\n",
      "3274/3274 [==============================] - 22s - loss: 170.5512 - val_loss: 493.0911\n",
      "Training duration (s) :  137.7429440021515\n",
      "Train the 83.h5 model\n",
      "> Compilation Time :  0.033158063888549805\n",
      "Train on 3259 samples, validate on 363 samples\n",
      "Epoch 1/5\n",
      "3259/3259 [==============================] - 37s - loss: 152.5156 - val_loss: 2.4475\n",
      "Epoch 2/5\n",
      "3259/3259 [==============================] - 22s - loss: 151.7619 - val_loss: 2.0496\n",
      "Epoch 3/5\n",
      "3259/3259 [==============================] - 21s - loss: 150.9061 - val_loss: 1.8388\n",
      "Epoch 4/5\n",
      "3259/3259 [==============================] - 21s - loss: 150.1731 - val_loss: 1.6538\n",
      "Epoch 5/5\n",
      "3259/3259 [==============================] - 21s - loss: 149.3052 - val_loss: 1.4945\n",
      "Training duration (s) :  142.75597190856934\n",
      "Train the 84.h5 model\n",
      "> Compilation Time :  0.027352094650268555\n",
      "Train on 2395 samples, validate on 267 samples\n",
      "Epoch 1/5\n",
      "2395/2395 [==============================] - 15s - loss: 2.1705 - val_loss: 0.7134\n",
      "Epoch 2/5\n",
      "2395/2395 [==============================] - 2s - loss: 1.8511 - val_loss: 0.5671\n",
      "Epoch 3/5\n",
      "2395/2395 [==============================] - 2s - loss: 1.7078 - val_loss: 0.5183\n",
      "Epoch 4/5\n",
      "2395/2395 [==============================] - 2s - loss: 1.6259 - val_loss: 0.5085\n",
      "Epoch 5/5\n",
      "2395/2395 [==============================] - 2s - loss: 1.5757 - val_loss: 0.4788\n",
      "Training duration (s) :  37.081342935562134\n",
      "Train the 85.h5 model\n",
      "> Compilation Time :  0.028252124786376953\n",
      "Train on 2389 samples, validate on 266 samples\n",
      "Epoch 1/5\n",
      "2389/2389 [==============================] - 16s - loss: 1.9004 - val_loss: 1.7588\n",
      "Epoch 2/5\n",
      "2389/2389 [==============================] - 2s - loss: 1.6694 - val_loss: 1.5103\n",
      "Epoch 3/5\n",
      "2389/2389 [==============================] - 2s - loss: 1.5309 - val_loss: 1.4099\n",
      "Epoch 4/5\n",
      "2389/2389 [==============================] - 2s - loss: 1.4528 - val_loss: 1.3633\n",
      "Epoch 5/5\n",
      "2389/2389 [==============================] - 2s - loss: 1.4127 - val_loss: 1.2913\n",
      "Training duration (s) :  37.38384985923767\n",
      "Train the 86.h5 model\n",
      "> Compilation Time :  0.02945399284362793\n",
      "Train on 2375 samples, validate on 264 samples\n",
      "Epoch 1/5\n",
      "2375/2375 [==============================] - 16s - loss: 1.9985 - val_loss: 1.1778\n",
      "Epoch 2/5\n",
      "2375/2375 [==============================] - 2s - loss: 1.8527 - val_loss: 1.0203\n",
      "Epoch 3/5\n",
      "2375/2375 [==============================] - 2s - loss: 1.6981 - val_loss: 0.9176\n",
      "Epoch 4/5\n",
      "2375/2375 [==============================] - 2s - loss: 1.5889 - val_loss: 0.8602\n",
      "Epoch 5/5\n",
      "2375/2375 [==============================] - 2s - loss: 1.5253 - val_loss: 0.8204\n",
      "Training duration (s) :  39.19588923454285\n",
      "Train the 87.h5 model\n",
      "> Compilation Time :  0.03390192985534668\n",
      "Train on 2368 samples, validate on 264 samples\n",
      "Epoch 1/5\n",
      "2368/2368 [==============================] - 19s - loss: 1.7130 - val_loss: 0.7311\n",
      "Epoch 2/5\n",
      "2368/2368 [==============================] - 5s - loss: 1.3888 - val_loss: 0.6488\n",
      "Epoch 3/5\n",
      "2368/2368 [==============================] - 5s - loss: 1.2606 - val_loss: 0.5620\n",
      "Epoch 4/5\n",
      "2368/2368 [==============================] - 5s - loss: 1.1974 - val_loss: 0.5392\n",
      "Epoch 5/5\n",
      "2368/2368 [==============================] - 5s - loss: 1.1383 - val_loss: 0.5302\n",
      "Training duration (s) :  62.10694098472595\n",
      "Train the 88.h5 model\n",
      "> Compilation Time :  0.027675867080688477\n",
      "Train on 2362 samples, validate on 263 samples\n",
      "Epoch 1/5\n",
      "2362/2362 [==============================] - 19s - loss: 1.9687 - val_loss: 1.0868\n",
      "Epoch 2/5\n",
      "2362/2362 [==============================] - 5s - loss: 1.6914 - val_loss: 0.8570\n",
      "Epoch 3/5\n",
      "2362/2362 [==============================] - 5s - loss: 1.4893 - val_loss: 0.7701\n",
      "Epoch 4/5\n",
      "2362/2362 [==============================] - 5s - loss: 1.3914 - val_loss: 0.7198\n",
      "Epoch 5/5\n",
      "2362/2362 [==============================] - 5s - loss: 1.3375 - val_loss: 0.6770\n",
      "Training duration (s) :  52.29593276977539\n",
      "Train the 89.h5 model\n",
      "> Compilation Time :  0.027382850646972656\n",
      "Train on 2348 samples, validate on 261 samples\n",
      "Epoch 1/5\n",
      "2348/2348 [==============================] - 19s - loss: 2.0364 - val_loss: 1.3599\n",
      "Epoch 2/5\n",
      "2348/2348 [==============================] - 5s - loss: 1.8220 - val_loss: 1.1340\n",
      "Epoch 3/5\n",
      "2348/2348 [==============================] - 5s - loss: 1.6254 - val_loss: 1.0767\n",
      "Epoch 4/5\n",
      "2348/2348 [==============================] - 5s - loss: 1.5168 - val_loss: 0.9997\n",
      "Epoch 5/5\n",
      "2348/2348 [==============================] - 5s - loss: 1.4582 - val_loss: 0.9654\n",
      "Training duration (s) :  52.30704998970032\n",
      "Train the 90.h5 model\n",
      "> Compilation Time :  0.027318239212036133\n",
      "Train on 2341 samples, validate on 261 samples\n",
      "Epoch 1/5\n",
      "2341/2341 [==============================] - 21s - loss: 2.2274 - val_loss: 1.2227\n",
      "Epoch 2/5\n",
      "2341/2341 [==============================] - 7s - loss: 1.8797 - val_loss: 0.9369\n",
      "Epoch 3/5\n",
      "2341/2341 [==============================] - 7s - loss: 1.6637 - val_loss: 0.8233\n",
      "Epoch 4/5\n",
      "2341/2341 [==============================] - 7s - loss: 1.5551 - val_loss: 0.7779\n",
      "Epoch 5/5\n",
      "2341/2341 [==============================] - 7s - loss: 1.5001 - val_loss: 0.7294\n",
      "Training duration (s) :  64.77665781974792\n",
      "Train the 91.h5 model\n",
      "> Compilation Time :  0.02735614776611328\n",
      "Train on 2335 samples, validate on 260 samples\n",
      "Epoch 1/5\n",
      "2335/2335 [==============================] - 22s - loss: 2.2269 - val_loss: 0.8774\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 7s - loss: 1.9526 - val_loss: 0.6816\n",
      "Epoch 3/5\n",
      "2335/2335 [==============================] - 7s - loss: 1.7163 - val_loss: 0.6552\n",
      "Epoch 4/5\n",
      "2335/2335 [==============================] - 7s - loss: 1.6119 - val_loss: 0.6024\n",
      "Epoch 5/5\n",
      "2335/2335 [==============================] - 7s - loss: 1.5694 - val_loss: 0.5998\n",
      "Training duration (s) :  66.20178604125977\n",
      "Train the 92.h5 model\n",
      "> Compilation Time :  0.028512001037597656\n",
      "Train on 2321 samples, validate on 258 samples\n",
      "Epoch 1/5\n",
      "2321/2321 [==============================] - 20s - loss: 2.1656 - val_loss: 1.8129\n",
      "Epoch 2/5\n",
      "2321/2321 [==============================] - 7s - loss: 1.9351 - val_loss: 1.5027\n",
      "Epoch 3/5\n",
      "2321/2321 [==============================] - 7s - loss: 1.7351 - val_loss: 1.3598\n",
      "Epoch 4/5\n",
      "2321/2321 [==============================] - 7s - loss: 1.6060 - val_loss: 1.2893\n",
      "Epoch 5/5\n",
      "2321/2321 [==============================] - 7s - loss: 1.5444 - val_loss: 1.2024\n",
      "Training duration (s) :  64.7746250629425\n",
      "Train the 93.h5 model\n",
      "> Compilation Time :  0.029247045516967773\n",
      "Train on 2260 samples, validate on 252 samples\n",
      "Epoch 1/5\n",
      "2260/2260 [==============================] - 31s - loss: 2.2167 - val_loss: 1.2111\n",
      "Epoch 2/5\n",
      "2260/2260 [==============================] - 15s - loss: 1.7498 - val_loss: 0.8582\n",
      "Epoch 3/5\n",
      "2260/2260 [==============================] - 14s - loss: 1.4786 - val_loss: 0.7863\n",
      "Epoch 4/5\n",
      "2260/2260 [==============================] - 16s - loss: 1.3402 - val_loss: 0.7669\n",
      "Epoch 5/5\n",
      "2260/2260 [==============================] - 16s - loss: 1.2892 - val_loss: 0.7467\n",
      "Training duration (s) :  110.4443039894104\n",
      "Train the 94.h5 model\n",
      "> Compilation Time :  0.027545928955078125\n",
      "Train on 2254 samples, validate on 251 samples\n",
      "Epoch 1/5\n",
      "2254/2254 [==============================] - 31s - loss: 2.0555 - val_loss: 1.5394\n",
      "Epoch 2/5\n",
      "2254/2254 [==============================] - 15s - loss: 1.6855 - val_loss: 1.1863\n",
      "Epoch 3/5\n",
      "2254/2254 [==============================] - 15s - loss: 1.4004 - val_loss: 1.1110\n",
      "Epoch 4/5\n",
      "2254/2254 [==============================] - 15s - loss: 1.2784 - val_loss: 1.0571\n",
      "Epoch 5/5\n",
      "2254/2254 [==============================] - 15s - loss: 1.2257 - val_loss: 1.0169\n",
      "Training duration (s) :  109.68081998825073\n",
      "Train the 95.h5 model\n",
      "> Compilation Time :  0.03485703468322754\n",
      "Train on 2240 samples, validate on 249 samples\n",
      "Epoch 1/5\n",
      "2240/2240 [==============================] - 32s - loss: 1.9315 - val_loss: 1.7723\n",
      "Epoch 2/5\n",
      "2240/2240 [==============================] - 16s - loss: 1.6648 - val_loss: 1.4567\n",
      "Epoch 3/5\n",
      "2240/2240 [==============================] - 15s - loss: 1.3926 - val_loss: 1.3825\n",
      "Epoch 4/5\n",
      "2240/2240 [==============================] - 14s - loss: 1.2741 - val_loss: 1.2594\n",
      "Epoch 5/5\n",
      "2240/2240 [==============================] - 16s - loss: 1.1981 - val_loss: 1.2071\n",
      "Training duration (s) :  112.73839616775513\n",
      "Total training duration (s) :  8719.424929141998\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lstm\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "ModelInformation = pd.read_pickle(\"../model/ModelInformation.pickle\")\n",
    "columns = ModelInformation.columns\n",
    "global_start_time = time.time()\n",
    "for i in range(len(ModelInformation)):\n",
    "    forloop_start_time = time.time()\n",
    "    #\n",
    "    Filename = ModelInformation.loc[i,columns[0]]\n",
    "    ColumnList = ModelInformation.loc[i,columns[1]]\n",
    "    WindowSize = ModelInformation.loc[i,columns[2]]\n",
    "    NumOfPredictDay = ModelInformation.loc[i,columns[3]]\n",
    "    \n",
    "    print('Train the ' + str(i) + '.h5 model')\n",
    "    \n",
    "    #載入資料\n",
    "    DataSet = lstm.LoadData(Filename, ColumnList, WindowSize, NumOfPredictDay)\n",
    "    #正規化資料\n",
    "    NormalizeData = lstm.NormaliseWindows(DataSet)\n",
    "    #切割資料\n",
    "    #x_train, y_train, x_test, y_test = lstm.SplitData(NormalizeData, ColumnList, NumOfPredictDay)\n",
    "    #sequence_length = WindowSize + NumOfPredictDay\n",
    "\n",
    "    #切割訓練資料\n",
    "    x_train, y_train = lstm.SplitDatatoTrain(NormalizeData, ColumnList, NumOfPredictDay)\n",
    "    #切割預測資料\n",
    "    x_predict = lstm.SplitDatatoPredict(DataSet, ColumnList, NumOfPredictDay)\n",
    "    x_predict = lstm.NormaliseWindows(x_predict)\n",
    "    \n",
    "    #\n",
    "    Layer = ModelInformation.loc[i,columns[4]]\n",
    "    Loss = ModelInformation.loc[i,columns[5]]\n",
    "    Optimizer = ModelInformation.loc[i,columns[6]]\n",
    "    \n",
    "    #建立LSTM模型\n",
    "    model = lstm.build_model(Layer,Loss,Optimizer)\n",
    "    \n",
    "    #\n",
    "    BatchSize = ModelInformation.loc[i,columns[7]]\n",
    "    Epoch = ModelInformation.loc[i,columns[8]]\n",
    "    ValidationSplit = ModelInformation.loc[i,columns[9]]\n",
    "    \n",
    "    #訓練LSTM模型\n",
    "    model.fit(  x_train,\n",
    "                y_train,\n",
    "                batch_size=BatchSize,\n",
    "                nb_epoch=Epoch,\n",
    "                validation_split=ValidationSplit)\n",
    "    \n",
    "    #\n",
    "    ModelName = ModelInformation.loc[i,columns[10]]\n",
    "    \n",
    "    #HDF5, pip3\n",
    "    model.save('../model/' + ModelName)\n",
    "    print('Training duration (s) : ',time.time() - forloop_start_time)\n",
    "\n",
    "print('Total training duration (s) : ',time.time() - global_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cbb67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = lstm.SplitDatatoTest(NormalizeData, ColumnList, NumOfPredictDay)\n",
    "#預測\n",
    "predictions = lstm.predict_point_by_point(model, x_test)\n",
    "lstm.plot_predict(predictions, NumOfPredictDay, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1505750400</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1505836800</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1505923200</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1506009600</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1506096000</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1506182400</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1506268800</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1506355200</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1506441600</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1506528000</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1506614400</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1506700800</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1506787200</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1506873600</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1506960000</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1507046400</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1507132800</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1507219200</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1507305600</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1507392000</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1507478400</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1507564800</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1507651200</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1507737600</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1507824000</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1507910400</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1507996800</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1508083200</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1508169600</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1508256000</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0     1     2     3     4     5\n",
       "0   1505750400  null  null  null  null  null\n",
       "1   1505836800  null  null  null  null  null\n",
       "2   1505923200  null  null  null  null  null\n",
       "3   1506009600  null  null  null  null  null\n",
       "4   1506096000  null  null  null  null  null\n",
       "5   1506182400  null  null  null  null  null\n",
       "6   1506268800  null  null  null  null  null\n",
       "7   1506355200  null  null  null  null  null\n",
       "8   1506441600  null  null  null  null  null\n",
       "9   1506528000  null  null  null  null  null\n",
       "10  1506614400  null  null  null  null  null\n",
       "11  1506700800  null  null  null  null  null\n",
       "12  1506787200  null  null  null  null  null\n",
       "13  1506873600  null  null  null  null  null\n",
       "14  1506960000  null  null  null  null  null\n",
       "15  1507046400  null  null  null  null  null\n",
       "16  1507132800  null  null  null  null  null\n",
       "17  1507219200  null  null  null  null  null\n",
       "18  1507305600  null  null  null  null  null\n",
       "19  1507392000  null  null  null  null  null\n",
       "20  1507478400  null  null  null  null  null\n",
       "21  1507564800  null  null  null  null  null\n",
       "22  1507651200  null  null  null  null  null\n",
       "23  1507737600  null  null  null  null  null\n",
       "24  1507824000  null  null  null  null  null\n",
       "25  1507910400  null  null  null  null  null\n",
       "26  1507996800  null  null  null  null  null\n",
       "27  1508083200  null  null  null  null  null\n",
       "28  1508169600  null  null  null  null  null\n",
       "29  1508256000  null  null  null  null  null"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "Today = datetime.datetime.today()\n",
    "Today += datetime.timedelta(1)\n",
    "PredictDay = []\n",
    "for i in range(30):\n",
    "    date = '{0}-{1}-{2}'.format(Today.year, Today.month, Today.day)\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    Today += datetime.timedelta(1)\n",
    "    \n",
    "    PredictDay.append(timeStamp)\n",
    "PredictDayofNULL = pd.DataFrame(columns=range(6), index=range(30), data='null')\n",
    "PredictDayofNULL.iloc[:,0] = PredictDay\n",
    "PredictDayofNULL.to_json('../json/PredictDayofNULL.json')\n",
    "PredictDayofNULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "DataList = os.listdir('../data')\n",
    "for CSV in DataList[0:-2]:\n",
    "    CSVFile = pd.read_csv('../data/' + CSV)\n",
    "    CSVFile.to_json('../json/' + CSV[:-4] + '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
